#!/usr/bin/env python3
"""
–ú–Ω–æ–≥–æ–ø—Ä–æ—Ü–µ—Å—Å–Ω–∞—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Å –Ω–∞—Å—Ç–æ—è—â–∏–º –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–æ–º
"""

import os
import sys
import time
import multiprocessing
from pathlib import Path
from typing import List, Dict, Any
import shutil
from tqdm import tqdm

# –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
sys.path.append(str(Path(__file__).parent.parent))

from src.processors.data_processor import LegalDocumentProcessor
from src.databases.vector_database import VectorDatabase
from src.utils.config import PDF_DIR, JSON_DIR, CHROMA_DB_PATH
from loguru import logger

def process_single_pdf_worker(pdf_file: str) -> Dict[str, Any]:
    """–†–∞–±–æ—á–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ PDF —Ñ–∞–π–ª–∞"""
    try:
        start_time = time.time()
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –≤ –∫–∞–∂–¥–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ
        processor = LegalDocumentProcessor(use_gemini_chunking=True)
        
        pdf_path = os.path.join(PDF_DIR, pdf_file)
        json_file = pdf_file.replace('.pdf', '.json')
        json_path = os.path.join(JSON_DIR, json_file)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Ñ–∞–π–ª
        if os.path.exists(json_path):
            try:
                if os.path.getmtime(json_path) >= os.path.getmtime(pdf_path):
                    return {
                        'file': pdf_file,
                        'status': 'skipped',
                        'chunks': 0,
                        'positions': 0,
                        'error': None,
                        'processing_time': 0
                    }
            except OSError:
                pass  # –§–∞–π–ª –ø–æ–≤—Ä–µ–∂–¥–µ–Ω, –ø–µ—Ä–µ–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª
        data = processor.process_pdf_to_json(pdf_path)
        if data:
            processor.save_json(data, json_path)
            processing_time = time.time() - start_time
            
            return {
                'file': pdf_file,
                'status': 'processed',
                'chunks': data.get('processing_info', {}).get('total_chunks', 0),
                'positions': data.get('processing_info', {}).get('total_positions', 0),
                'error': None,
                'processing_time': processing_time
            }
        else:
            return {
                'file': pdf_file,
                'status': 'error',
                'chunks': 0,
                'positions': 0,
                'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ñ–∞–π–ª',
                'processing_time': time.time() - start_time
            }
            
    except Exception as e:
        return {
            'file': pdf_file,
            'status': 'error',
            'chunks': 0,
            'positions': 0,
            'error': str(e),
            'processing_time': time.time() - start_time if 'start_time' in locals() else 0
        }

def clear_old_data():
    """–û—á–∏—â–∞–µ—Ç —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ"""
    logger.info("üóëÔ∏è –û—á–∏—â–∞—é —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ...")
    
    # –û—á–∏—â–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É
    if os.path.exists(CHROMA_DB_PATH):
        logger.info(f"üóëÔ∏è –£–¥–∞–ª—è—é —Å—Ç–∞—Ä—É—é –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É: {CHROMA_DB_PATH}")
        shutil.rmtree(CHROMA_DB_PATH)
    
    # –û—á–∏—â–∞–µ–º JSON —Ñ–∞–π–ª—ã
    if os.path.exists(JSON_DIR):
        logger.info(f"üóëÔ∏è –£–¥–∞–ª—è—é —Å—Ç–∞—Ä—ã–µ JSON —Ñ–∞–π–ª—ã: {JSON_DIR}")
        shutil.rmtree(JSON_DIR)
        os.makedirs(JSON_DIR, exist_ok=True)

def get_pdf_files() -> List[str]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ PDF —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    if not os.path.exists(PDF_DIR):
        logger.error(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {PDF_DIR} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
        return []
    
    pdf_files = [f for f in os.listdir(PDF_DIR) if f.lower().endswith('.pdf')]
    logger.info(f"üìÑ –ù–∞–π–¥–µ–Ω–æ {len(pdf_files)} PDF —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
    return pdf_files

def process_pdfs_multiprocess(pdf_files: List[str], num_processes: int = None) -> List[Dict[str, Any]]:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç PDF —Ñ–∞–π–ª—ã –≤ –º–Ω–æ–≥–æ–ø—Ä–æ—Ü–µ—Å—Å–Ω–æ–º —Ä–µ–∂–∏–º–µ"""
    if num_processes is None:
        num_processes = min(multiprocessing.cpu_count(), len(pdf_files))
    
    logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞—é –º–Ω–æ–≥–æ–ø—Ä–æ—Ü–µ—Å—Å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É {len(pdf_files)} —Ñ–∞–π–ª–æ–≤")
    logger.info(f"‚ö° –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {num_processes} –ø—Ä–æ—Ü–µ—Å—Å–æ–≤")
    
    start_time = time.time()
    
    # –°–æ–∑–¥–∞–µ–º –ø—É–ª –ø—Ä–æ—Ü–µ—Å—Å–æ–≤
    with multiprocessing.Pool(processes=num_processes) as pool:
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª—ã —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
        results = []
        with tqdm(total=len(pdf_files), desc="üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ PDF", unit="—Ñ–∞–π–ª") as pbar:
            for result in pool.imap(process_single_pdf_worker, pdf_files):
                results.append(result)
                pbar.update(1)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
                if result['status'] == 'processed':
                    pbar.set_postfix({
                        'chunks': result['chunks'],
                        'positions': result['positions'],
                        'time': f"{result['processing_time']:.1f}s"
                    })
                elif result['status'] == 'skipped':
                    pbar.set_postfix({'status': 'skipped'})
                else:
                    pbar.set_postfix({'status': 'error'})
    
    total_time = time.time() - start_time
    logger.info(f"‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {total_time:.2f}—Å")
    
    return results

def load_to_vector_database():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ JSON —Ñ–∞–π–ª—ã –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É"""
    logger.info("üìö –ó–∞–≥—Ä—É–∂–∞—é –¥–∞–Ω–Ω—ã–µ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É...")
    
    try:
        vector_db = VectorDatabase()
        vector_db.initialize_database()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º JSON —Ñ–∞–π–ª—ã
        stats = vector_db.load_from_json_files()
        
        logger.info("‚úÖ –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∞:")
        logger.info(f"   üìÑ –ù–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤: {stats['new_files']}")
        logger.info(f"   üìö –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {stats['loaded']}")
        logger.info(f"   ‚è≠Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ: {stats['skipped']}")
        logger.info(f"   ‚ùå –û—à–∏–±–æ–∫: {stats['errors']}")
        logger.info(f"   üìä –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {stats['total_chunks']}")
        logger.info(f"   üìç –í—Å–µ–≥–æ –ø–æ–∑–∏—Ü–∏–π: {stats['total_positions']}")
        
        return stats
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É: {e}")
        return None

def print_statistics(results: List[Dict[str, Any]]):
    """–í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    processed = sum(1 for r in results if r['status'] == 'processed')
    skipped = sum(1 for r in results if r['status'] == 'skipped')
    errors = sum(1 for r in results if r['status'] == 'error')
    
    total_chunks = sum(r['chunks'] for r in results)
    total_positions = sum(r['positions'] for r in results)
    
    avg_time = sum(r['processing_time'] for r in results if r['processing_time'] > 0) / max(processed, 1)
    
    logger.info("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–ë–†–ê–ë–û–¢–ö–ò:")
    logger.info(f"   ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed}")
    logger.info(f"   ‚è≠Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ: {skipped}")
    logger.info(f"   ‚ùå –û—à–∏–±–æ–∫: {errors}")
    logger.info(f"   üìÑ –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {len(results)}")
    logger.info(f"   üìö –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {total_chunks}")
    logger.info(f"   üìç –í—Å–µ–≥–æ –ø–æ–∑–∏—Ü–∏–π: {total_positions}")
    logger.info(f"   ‚è±Ô∏è –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ —Ñ–∞–π–ª: {avg_time:.2f}—Å")
    
    if errors > 0:
        logger.warning("‚ùå –§–∞–π–ª—ã —Å –æ—à–∏–±–∫–∞–º–∏:")
        for result in results:
            if result['status'] == 'error':
                logger.warning(f"   - {result['file']}: {result['error']}")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –º–Ω–æ–≥–æ–ø—Ä–æ—Ü–µ—Å—Å–Ω–æ–π –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
    
    # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ
    clear_old_data()
    
    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ PDF —Ñ–∞–π–ª–æ–≤
    pdf_files = get_pdf_files()
    if not pdf_files:
        logger.error("PDF —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º PDF —Ñ–∞–π–ª—ã –≤ –º–Ω–æ–≥–æ–ø—Ä–æ—Ü–µ—Å—Å–Ω–æ–º —Ä–µ–∂–∏–º–µ
    results = process_pdfs_multiprocess(pdf_files)
    
    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    print_statistics(results)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É
    vector_stats = load_to_vector_database()
    
    if vector_stats:
        logger.info("üéâ –ü–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
    else:
        logger.error("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É")

if __name__ == "__main__":
    main()
