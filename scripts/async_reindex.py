#!/usr/bin/env python3
"""
–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º –¥–ª—è –±–æ–ª—å—à–∏—Ö –∫–æ—Ä–ø—É—Å–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
"""

import os
import sys
import asyncio
import time
from pathlib import Path
from typing import List, Dict, Any
from concurrent.futures import ThreadPoolExecutor
import shutil

# –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
sys.path.append(str(Path(__file__).parent.parent))

from src.processors.data_processor import LegalDocumentProcessor
from src.databases.vector_database import VectorDatabase
from src.utils.config import PDF_DIR, JSON_DIR, CHROMA_DB_PATH
from loguru import logger
from tqdm.asyncio import tqdm

class AsyncReindexer:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º"""
    
    def __init__(self, max_workers: int = 8):
        self.max_workers = max_workers
        self.processor = LegalDocumentProcessor(use_gemini_chunking=True)
        self.vector_db = None
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        
    async def clear_old_data(self):
        """–û—á–∏—â–∞–µ—Ç —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ"""
        logger.info("üóëÔ∏è –û—á–∏—â–∞—é —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ...")
        
        # –û—á–∏—â–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É
        if os.path.exists(CHROMA_DB_PATH):
            logger.info(f"üóëÔ∏è –£–¥–∞–ª—è—é —Å—Ç–∞—Ä—É—é –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É: {CHROMA_DB_PATH}")
            shutil.rmtree(CHROMA_DB_PATH)
        
        # –û—á–∏—â–∞–µ–º JSON —Ñ–∞–π–ª—ã
        if os.path.exists(JSON_DIR):
            logger.info(f"üóëÔ∏è –£–¥–∞–ª—è—é —Å—Ç–∞—Ä—ã–µ JSON —Ñ–∞–π–ª—ã: {JSON_DIR}")
            shutil.rmtree(JSON_DIR)
            os.makedirs(JSON_DIR, exist_ok=True)
    
    async def get_pdf_files(self) -> List[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ PDF —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        if not os.path.exists(PDF_DIR):
            logger.error(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {PDF_DIR} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
            return []
        
        pdf_files = [f for f in os.listdir(PDF_DIR) if f.lower().endswith('.pdf')]
        logger.info(f"üìÑ –ù–∞–π–¥–µ–Ω–æ {len(pdf_files)} PDF —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        return pdf_files
    
    async def process_single_pdf(self, pdf_file: str) -> Dict[str, Any]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω PDF —Ñ–∞–π–ª"""
        start_time = time.time()
        try:
            pdf_path = os.path.join(PDF_DIR, pdf_file)
            json_file = pdf_file.replace('.pdf', '.json')
            json_path = os.path.join(JSON_DIR, json_file)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Ñ–∞–π–ª
            if os.path.exists(json_path):
                try:
                    if os.path.getmtime(json_path) >= os.path.getmtime(pdf_path):
                        return {
                            'file': pdf_file,
                            'status': 'skipped',
                            'chunks': 0,
                            'positions': 0,
                            'error': None,
                            'processing_time': 0
                        }
                except OSError:
                    pass  # –§–∞–π–ª –ø–æ–≤—Ä–µ–∂–¥–µ–Ω, –ø–µ—Ä–µ–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                self.executor,
                self._process_pdf_sync,
                pdf_path
            )
            
            processing_time = time.time() - start_time
            result['processing_time'] = processing_time
            logger.info(f"‚úÖ {pdf_file} –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∑–∞ {processing_time:.2f}—Å")
            
            return {
                'file': pdf_file,
                'status': 'processed',
                'chunks': result.get('processing_info', {}).get('total_chunks', 0),
                'positions': result.get('processing_info', {}).get('total_positions', 0),
                'error': None
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {pdf_file}: {e}")
            return {
                'file': pdf_file,
                'status': 'error',
                'chunks': 0,
                'positions': 0,
                'error': str(e)
            }
    
    def _process_pdf_sync(self, pdf_path: str) -> Dict[str, Any]:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ PDF —Ñ–∞–π–ª–∞"""
        return self.processor.process_pdf_to_json(pdf_path)
    
    async def process_all_pdfs_async(self) -> Dict[str, Any]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ PDF —Ñ–∞–π–ª—ã"""
        pdf_files = await self.get_pdf_files()
        
        if not pdf_files:
            return {
                'processed': 0,
                'skipped': 0,
                'errors': 0,
                'total_chunks': 0,
                'total_positions': 0,
                'processed_files': []
            }
        
        logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞—é –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É {len(pdf_files)} —Ñ–∞–π–ª–æ–≤")
        logger.info(f"‚ö° –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {self.max_workers} –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤")
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        tasks = [self.process_single_pdf(pdf_file) for pdf_file in pdf_files]
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
        results = []
        with tqdm(total=len(tasks), desc="üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ PDF", unit="—Ñ–∞–π–ª") as pbar:
            for coro in asyncio.as_completed(tasks):
                result = await coro
                results.append(result)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
                if result['status'] == 'processed':
                    pbar.set_postfix({
                        'chunks': result['chunks'],
                        'positions': result['positions']
                    })
                elif result['status'] == 'skipped':
                    pbar.set_postfix({'status': 'skipped'})
                elif result['status'] == 'error':
                    pbar.set_postfix({'status': 'error'})
                
                pbar.update(1)
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        processed = sum(1 for r in results if r['status'] == 'processed')
        skipped = sum(1 for r in results if r['status'] == 'skipped')
        errors = sum(1 for r in results if r['status'] == 'error')
        total_chunks = sum(r['chunks'] for r in results)
        total_positions = sum(r['positions'] for r in results)
        processed_files = [r['file'] for r in results if r['status'] == 'processed']
        
        logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞:")
        logger.info(f"   üìÑ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed} —Ñ–∞–π–ª–æ–≤")
        logger.info(f"   ‚è≠Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ: {skipped} —Ñ–∞–π–ª–æ–≤")
        logger.info(f"   ‚ùå –û—à–∏–±–æ–∫: {errors} —Ñ–∞–π–ª–æ–≤")
        logger.info(f"   üìä –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {total_chunks}")
        logger.info(f"   üìä –í—Å–µ–≥–æ –ø–æ–∑–∏—Ü–∏–π: {total_positions}")
        
        return {
            'processed': processed,
            'skipped': skipped,
            'errors': errors,
            'total_chunks': total_chunks,
            'total_positions': total_positions,
            'processed_files': processed_files
        }
    
    async def create_vector_database(self):
        """–°–æ–∑–¥–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        logger.info("üîç –°–æ–∑–¥–∞—é –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö...")
        
        loop = asyncio.get_event_loop()
        self.vector_db = await loop.run_in_executor(
            self.executor,
            VectorDatabase
        )
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º JSON —Ñ–∞–π–ª—ã –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É
        json_files = [f for f in os.listdir(JSON_DIR) if f.endswith('.json')]
        
        if not json_files:
            logger.warning("JSON —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return
        
        logger.info(f"üìö –ó–∞–≥—Ä—É–∂–∞—é {len(json_files)} JSON —Ñ–∞–π–ª–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª—ã —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
        with tqdm(total=len(json_files), desc="üìö –ó–∞–≥—Ä—É–∑–∫–∞ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î", unit="—Ñ–∞–π–ª") as pbar:
            for json_file in json_files:
                json_path = os.path.join(JSON_DIR, json_file)
                
                try:
                    await loop.run_in_executor(
                        self.executor,
                        self._load_single_json,
                        json_path
                    )
                    pbar.update(1)
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {json_file}: {e}")
                    pbar.update(1)
        
        logger.info("‚úÖ –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —Å–æ–∑–¥–∞–Ω–∞")
    
    def _load_single_json(self, json_path: str):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–¥–∏–Ω JSON —Ñ–∞–π–ª –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É"""
        import json
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É
        self.vector_db.add_documents([data])
    
    async def test_search(self):
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø–æ–∏—Å–∫ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ"""
        if not self.vector_db:
            logger.error("–í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ –Ω–µ —Å–æ–∑–¥–∞–Ω–∞")
            return
        
        logger.info("üîç –¢–µ—Å—Ç–∏—Ä—É—é –ø–æ–∏—Å–∫ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ...")
        
        test_queries = [
            "–∑–∞—â–∏—Ç–∞ –ø—Ä–∞–≤ –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª–µ–π",
            "—Ä–∞—Å—Ç–æ—Ä–∂–µ–Ω–∏–µ –¥–æ–≥–æ–≤–æ—Ä–∞",
            "–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ–∫ —Ç–æ–≤–∞—Ä–∞",
            "–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω–∞—è –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å"
        ]
        
        for query in test_queries:
            try:
                results = self.vector_db.search_similar(query, n_results=3)
                logger.info(f"üîç –ó–∞–ø—Ä–æ—Å: '{query}' -> –Ω–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                
                for i, result in enumerate(results[:2], 1):
                    logger.info(f"   {i}. {result.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')[:50]}...")
                    
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –¥–ª—è '{query}': {e}")
    
    async def cleanup(self):
        """–û—á–∏—â–∞–µ—Ç —Ä–µ—Å—É—Ä—Å—ã"""
        if self.executor:
            self.executor.shutdown(wait=True)
    
    async def run_full_reindex(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—É—é –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é"""
        start_time = time.time()
        
        try:
            # 1. –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ
            await self.clear_old_data()
            
            # 2. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º PDF —Ñ–∞–π–ª—ã
            pdf_results = await self.process_all_pdfs_async()
            
            # 3. –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É
            await self.create_vector_database()
            
            # 4. –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫
            await self.test_search()
            
            # 5. –í—ã–≤–æ–¥–∏–º –∏—Ç–æ–≥–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            total_time = time.time() - start_time
            logger.info("üéâ –ü–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
            logger.info(f"‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.2f} —Å–µ–∫—É–Ω–¥")
            logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
            logger.info(f"   üìÑ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {pdf_results['processed']}")
            logger.info(f"   üìä –°–æ–∑–¥–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {pdf_results['total_chunks']}")
            logger.info(f"   üìä –ò–∑–≤–ª–µ—á–µ–Ω–æ –ø–æ–∑–∏—Ü–∏–π: {pdf_results['total_positions']}")
            logger.info(f"   ‚ö° –°–∫–æ—Ä–æ—Å—Ç—å: {pdf_results['processed']/total_time:.2f} —Ñ–∞–π–ª–æ–≤/—Å–µ–∫")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {e}")
            raise
        finally:
            await self.cleanup()

async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ CPU
    import multiprocessing
    max_workers = min(multiprocessing.cpu_count(), 8)  # –ú–∞–∫—Å–∏–º—É–º 8 –ø–æ—Ç–æ–∫–æ–≤
    
    reindexer = AsyncReindexer(max_workers=max_workers)
    
    try:
        await reindexer.run_full_reindex()
    except KeyboardInterrupt:
        logger.info("‚èπÔ∏è –ü–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
    finally:
        await reindexer.cleanup()

if __name__ == "__main__":
    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    logger.add("./logs/async_reindex.log", rotation="10 MB", level="INFO")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é
    asyncio.run(main())
